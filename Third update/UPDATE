In this update, I add a function to split train and test dataset. The proportion of training set is configurable, default is 0.8.
Four fine-tuned GPT2 models were trained. They are 124M_HarryPotter, 124M_dataset_top1000, 355M_dataset_top1000 and 124M_dataset_scores_over1000. I didn't upload them to Github because they are too large. After training, the loss of 124M_HarryPotter is around 0.6, and the loss of the remaining models are all around 2.0
Code has been updated, user can set length of generated story, temperature and the starting sentense of the generated story. User can also set the number of generated sample stories simutaneously. The generated stories will be saved in text file with timestamp.
statistics & visualization code have been created. They will do data analysis for generated stories. The number of noun, verb, adj and adv will be count and saved in a csv file. Some figures like the pie chart of wordcount and keyword candidates also be generated. New content is also appended to the text file so that user can know the total number of word and keyword candidates here.
Issue is that the evaluation of the trained models is hard because they are not in same format with those models in huggingface.
